# Codex Collaborative Solver V3.0

**Quality-First Debate Architecture with Self-Improving Facilitator**

## Overview

V3.0 introduces a fundamental paradigm shift: from "facilitating debates" to "ensuring quality outcomes through intelligent facilitation." This version features a 3-layer hybrid facilitator system, automated knowledge extraction via playbooks, and adaptive quality modes.

**Key Innovation:** Meta-monitoring layer that detects coverage gaps, anti-patterns, and information scarcity, automatically improving debate quality over time.

## When to Use This Skill

Activate when users request:
- "codexì™€ í† ë¡ í•´ì„œ í•´ê²°í•´ì¤˜" / "debate with codex to solve this"
- "claudeë‘ codex í˜‘ì—…ìœ¼ë¡œ ë‹µ ì°¾ì•„ì¤˜" / "claude and codex collaborate"
- "ë‘ AIì˜ ì˜ê²¬ì„ ë“¤ì–´ë³´ê³  ì‹¶ì–´" / "want opinions from both AIs"
- Complex problems requiring multi-perspective analysis

**Appropriate problem types:**

1. **Architecture Decisions** - "Should we use microservices or monolith?"
2. **Bug Investigation** - "What's causing this memory leak?"
3. **Algorithm Optimization** - "Can we improve from O(nÂ²) to O(n log n)?"
4. **Design Pattern Selection** - "Which design pattern fits this scenario?"

## Prerequisites

**Required:**
- OpenAI Codex CLI installed: `npm i -g @openai/codex`
- ChatGPT Plus/Pro subscription
- Codex authenticated: Run `codex` once

**Verify installation:**
```bash
codex --version
```

## What's New in V3.0

### ğŸ¯ Core Improvements

| Feature | V2.0 | V3.0 |
|---------|------|------|
| **Quality Assurance** | Manual | 3-layer Facilitator (auto) |
| **Knowledge Reuse** | None | Automated Playbook Pipeline |
| **Adaptability** | Fixed process | 3 Quality Modes |
| **Evidence** | Implicit | Tiered (with confidence) |
| **Coverage** | Ad-hoc | 8-dimension monitoring |
| **Failure Detection** | None | 10 explicit mechanisms |

### ğŸ“Š Expected Quality Gains

- **Actionability**: 60% â†’ 90%
- **Confidence Clarity**: 10% â†’ 100%
- **Coverage Completeness**: 4/8 â†’ 7/8 dimensions
- **Appropriate Pauses**: 0% â†’ 20-30% (when info missing)

## V3.0 Architecture

### 1. Hybrid Facilitator System (THE KEYSTONE)

**Why it matters:** Without meta-monitoring, all other improvements fail. The facilitator is the quality control layer.

#### Layer 1: Rule-Based Monitor (Every Round)

**Coverage Tracker**
- Monitors 8 dimensions: {architecture, security, performance, UX, testing, ops, cost, compliance}
- After each round, flags uncovered areas
- Example: "Neither agent addressed testing strategy - should we explore this?"

**Anti-Pattern Detector**
- Circular reasoning (same point repeated 2+ times)
- Premature convergence (agreement in <2 rounds without exploring alternatives)
- Information starvation (agents guessing instead of asking user)
- Dominance (one agent's view accepted without challenge)

**Scarcity Detector**
- Tracks assumption:fact ratio
- Counts critical unknowns
- **Abort Thresholds:**
  - â‰¥2 critical decision axes unknown after exploration pass
  - OR assumption:fact ratio > 2:1
  - Logs specific missing facts for re-engagement

**Mode Validator**
- Re-scores debate intent after each round
- Auto-switches modes if confidence drops
- Prevents mode misclassification

Configuration: `facilitator/rules/`

#### Layer 2: AI Escalation (On Flags)

Lightweight third AI agent handles non-trivial situations:
- "Circular reasoning detected between rounds 2-3" â†’ Suggests pivot to unexplored aspect
- "Neither agent addressed compliance" â†’ Prompts regulatory exploration
- High-friction patterns (rapid-turn, high disagreement, policy triggers) â†’ Escalates to user

Prompts: `facilitator/prompts/ai-escalation.md`

#### Layer 3: Quality Gate (Pre-Finalization)

Mandatory checklist before finalizing:
- [ ] Verified assumptions or marked as assumptions?
- [ ] User constraints honored?
- [ ] Risks surfaced with mitigation?
- [ ] Next actions concrete and executable?
- [ ] Confidence level explicit?

Template: `facilitator/quality-gate.md`

### 2. Quality Mode System

Users can specify (or auto-detected from keywords):

```yaml
Mode: exploration
  Purpose: Creative idea generation
  Claude weighting: Breadth + creativity (high)
  Codex weighting: Feasibility checks (soft)
  Rounds: 5-7
  Use when: "Explore architecture options", "Brainstorm approaches"

Mode: balanced (default)
  Purpose: Balanced decision-making
  Claude weighting: Standard
  Codex weighting: Standard
  Rounds: 3-5
  Use when: "Choose technology stack", "Architecture decision"

Mode: execution
  Purpose: Fast, actionable answers
  Claude weighting: User alignment
  Codex weighting: Implementation detail (high)
  Rounds: 2-3
  Use when: "Fix this bug", "Quick implementation approach"
```

Mode definitions: `modes/`

### 3. Automated Playbook Pipeline

**Turning Debates into Organizational Knowledge**

```
Debate Session
    â†“
Structured Logs (problem, assumptions, options, decisions)
    â†“
Auto-Tagging (shallow NLP, taxonomy labels)
    â†“
Nightly Clustering (similar problem signatures)
    â†“
Success Metrics Promotion (repeated patterns)
    â†“
LLM Summarizer (drafts playbook, cites sources)
    â†“
Human/Agent Validation (spot-check)
    â†“
Canonical Playbooks (60-day validity)
```

**Playbook Structure:**
- Problem signature (when to apply)
- Key questions to explore
- Common tradeoffs
- Evidence sources
- Past debate references
- Success metrics
- Expiration date

Examples: `playbooks/`
Template: `playbooks/_template.md`

### 4. Role Division

**Exploration Phase (Rounds 1-2):**
- **Claude**: Generate 3-5 diverse approaches (including wild ones)
- **Codex**: Reality-check feasibility ("This won't work because...")
- **Facilitator**: Ensure edge cases explored

**Convergence Phase (Rounds 3-5):**
- **Claude**: User alignment verification ("Solves actual problem?")
- **Codex**: Implementation feasibility ("Can we build this?")
- **Facilitator**: Force stress test before consensus

**Stress Pass (Before Consensus):**
- Last endorsing agent must enumerate failure modes
- No rubber-stamp escapes allowed

### 5. Tiered Evidence System

**Tier 1: Direct Evidence (High Confidence 90-100%)**
- Repo artifacts, actual code examples
- Real benchmarks, metrics
- Documented case studies
- Verifiable facts

**Tier 2: Reasoned Analogies (Medium Confidence 60-80%)**
- Similar problems, extrapolated patterns
- Industry best practices
- Documented tradeoffs
- Expert consensus

**Tier 3: Explicit Assumptions (Low Confidence 30-50%)**
- Clearly marked as assumptions
- Justified reasoning provided
- Confidence penalty applied
- Requires validation

**Never:** Fabricate data or present speculation as fact

### 6. Information Scarcity Protocol

**Fallback Chain:**
```
Evidence unavailable?
  â†“
1. Derivable from first principles? â†’ Reasoned analogy (Tier 2)
  â†“
2. User can answer? â†’ Ask clarifying question (PAUSE)
  â†“
3. Quick prototype/test possible? â†’ Suggest experiment
  â†“
4. Last resort: Make assumption + mark clearly + Tier 3 + confidence penalty
```

**When to ABORT and query user:**
- â‰¥2 critical decision axes unknown after exploration pass
- OR assumption:fact ratio > 2:1
- Facilitator logs which missing facts block risk evaluation
- Re-engagement prompt is specific

### 7. Edge Case Mitigations (10 Failure Modes)

1. **Circular Reasoning** â†’ Anti-pattern detector flags â†’ AI facilitator suggests pivot
2. **Premature Convergence** â†’ Requires 2+ rounds + alternative exploration
3. **Information Starvation** â†’ Scarcity detector â†’ abort and query user
4. **Dominance** â†’ Balance checker â†’ prompt underrepresented agent
5. **Mode Misclassification** â†’ Post-round re-scoring â†’ auto-switch
6. **Coverage Drift** â†’ Nightly telemetry â†’ propose new dimensions
7. **Playbook Staleness** â†’ 60-day expiry â†’ re-validation required
8. **Facilitator Myopia** â†’ High-friction patterns â†’ escalate to user
9. **Log Integrity** â†’ Heartbeat + replay buffer â†’ graceful pause
10. **Facilitator Failure** â†’ 7-round limit, 2-ignore threshold â†’ user override

## V3.0 Workflow

### Pre-Debate

1. **Clarification Stage** ğŸ†•
   - **Purpose**: Reduce assumptions, establish constraints, align on goals
   - **When**: Always (unless `--skip-clarify`)
   - **Process**:
     ```
     User request â†’ Claude analyzes complexity â†’ Generate 1-3 questions â†’ User answers â†’ Debate starts
     ```

   **Complexity Judgment (Agent-Driven)**:
   - Claude reads the user's question
   - Assesses: Are constraints mentioned? Is goal clear? Single or multi-dimensional?
   - Decides: How many clarification questions needed (0-3)

   **Question Categories**:
   - **Essential** (always ask if missing):
     - Constraints (tech stack, budget, timeline, team capability)
     - Goals & Success Criteria (what defines "solved"?)
     - Context (current system, why this problem matters)

   - **Conditional** (based on problem type):
     - Performance: Target metrics, current profiling data
     - Architecture: Existing system, integration concerns
     - Security: Compliance requirements, threat model
     - Bug: Reproduction steps, error logs

   **Example Flow**:
   ```
   User: "Django API ì‘ë‹µì´ ë„ˆë¬´ ëŠë ¤"

   Claude (internal judgment):
   - No constraints mentioned â†’ Ask
   - No target metrics â†’ Ask
   - Context unclear â†’ Ask
   â†’ Generate 3 questions

   Claude: "ëª…í™•í™” ì§ˆë¬¸:
   1. í˜„ì¬ ì‘ë‹µ ì‹œê°„ê³¼ ëª©í‘œ ì‘ë‹µ ì‹œê°„ì€?
   2. ì‚¬ìš© ì¤‘ì¸ Django ë²„ì „, DB, ìºì‹œ ìŠ¤íƒì€?
   3. ì˜ˆì‚°/ì‹œê°„/íŒ€ ì œì•½ì‚¬í•­ì€?"

   User: "í˜„ì¬ 2ì´ˆ, ëª©í‘œ 500ms. Django 4.2, PostgreSQL, Redis ì—†ìŒ. 1ì£¼ì¼ ë‚´ ê°œì„ ."

   â†’ Now debate with full context
   ```

   **Skip Option**:
   ```
   User: "Django API ì„±ëŠ¥ ê°œì„  (í˜„ì¬ 2ì´ˆ â†’ ëª©í‘œ 500ms, Django 4.2/PostgreSQL, 1ì£¼ì¼)"
   â†’ Claude: (ëª¨ë“  ì •ë³´ ìˆìŒ) â†’ Skip clarify, start debate

   Or explicitly:
   User: "--skip-clarify Django API ì„±ëŠ¥ ê°œì„ "
   â†’ Claude: Debate immediately
   ```

   **Philosophy Alignment**:
   - âœ… "Scripts Assist, Agents Judge": Agent decides if/what to ask
   - âœ… No keyword matching: Natural language understanding
   - âœ… No rigid templates: Dynamic question generation
   - âœ… User control preserved: `--skip-clarify` flag

2. **Mode Selection**
   - User specifies: "Use exploration mode"
   - OR auto-detect from keywords
   - Default: balanced

3. **Playbook Loading**
   - Facilitator checks if relevant playbook exists
   - Loads playbook if available
   - Example: "database-migration.md" for DB questions

4. **Coverage Initialization**
   - Coverage monitor initializes 8-dimension checklist
   - Sets mode-specific thresholds
   - **Enhancement**: Uses clarification answers to set expected evidence tiers

### Mid-Debate User Input ğŸ†•

**Purpose:** Intelligent, agent-driven user engagement during debate when clarification or decision input would significantly improve the solution.

**Philosophy:** Hybrid approach combining Smart Prompting (default) with Manual Flag (power users).

#### When Claude Asks for Input

**4 Trigger Conditions:**

1. **Information Deficit** - Both Claude and Codex express low confidence (<50%) due to missing information
2. **Preference Fork** - Clear trade-off requires user priority judgment (e.g., performance vs maintainability)
3. **New Constraint Discovery** - Debate reveals constraint not mentioned in pre-clarification
4. **Long-Running Deadlock** - 3+ rounds without convergence, user needed as tie-breaker

**NOT Asked When:**
- High confidence (>70%) and converging
- Asked user within last 5 minutes
- Trivial implementation details
- Pre-clarification already covered topic

#### User Control Options

**CLI Flags:**
```bash
# Disable mid-debate input (fully automatic)
codexì™€ í† ë¡ í•´ì¤˜ --no-mid-input

# Force interactive mode (ask after every round)
codexì™€ í† ë¡ í•´ì¤˜ --interactive

# Tune frequency
codexì™€ í† ë¡ í•´ì¤˜ --mid-input-frequency=minimal  # Rarely ask
codexì™€ í† ë¡ í•´ì¤˜ --mid-input-frequency=balanced  # Default
codexì™€ í† ë¡ í•´ì¤˜ --mid-input-frequency=frequent  # Proactive
```

**Global Settings:**
```json
// ~/.claude/settings.json
{
  "codex_debate": {
    "mid_debate_input": {
      "enabled": true,
      "default_frequency": "balanced",
      "min_interval": 300
    }
  }
}
```

#### Example Flow

```markdown
## Round 2 Complete

Heuristic evaluation:
- Condition: Preference Fork detected (performance vs flexibility)
- Confidence: Claude 55%, Codex 60%
- Decision: Ask user

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ¤” Mid-Debate User Input Required     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ **Round:** 2 / 5                       â•‘
â•‘ **Type:** Preference Needed            â•‘
â•‘                                        â•‘
â•‘ **Current Discussion:**                â•‘
â•‘ We're evaluating eager loading vs      â•‘
â•‘ lazy+caching for Django ORM.           â•‘
â•‘                                        â•‘
â•‘ **Why We're Asking:**                  â•‘
â•‘ Both valid, but optimize for different â•‘
â•‘ goals. Need your priority.             â•‘
â•‘                                        â•‘
â•‘ **Question:** What's most important?   â•‘
â•‘                                        â•‘
â•‘ **Options:**                           â•‘
â•‘ 1. Performance (faster, simpler)       â•‘
â•‘ 2. Flexibility (more complex)          â•‘
â•‘ 3. Balanced approach                   â•‘
â•‘ 4. Skip (we'll decide)                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User: "1 - Performance"

âœ“ Thank you! Based on your priority (Performance),
  we'll proceed with eager loading approach.

Continuing debate with this context...
```

#### Integration with Workflow

**User response is:**
1. **Captured** - Question + answer logged
2. **Injected** - Next Codex round includes user input
3. **Tracked** - Prevents re-asking same question
4. **Referenced** - Included in final debate report

**Example injection to Codex (Round 3):**
```
User clarified (Round 2): Performance is the priority.

Given this preference, please re-evaluate your position...
```

#### Resources

- **Heuristic Guide:** `facilitator/mid-debate-heuristic.md`
- **Prompt Templates:** `facilitator/prompts/mid-debate-user-input.md`
- **CLI Flags:** `facilitator/cli-flags.md`
- **Integration:** `facilitator/reasoning-log-integration.md`

#### Benefits

- **Fewer Assumptions:** Fill gaps discovered during debate
- **Better Alignment:** Solutions match actual priorities
- **Faster Convergence:** Break deadlocks immediately
- **Higher Confidence:** User validation reduces uncertainty

#### Relationship to Pre-Clarification

**Complementary roles:**
- **Pre-clarification:** Initial context (tech stack, goals, constraints)
- **Mid-debate input:** Dynamic adjustments (priorities, trade-offs, new constraints)

**Synergy:**
- Good pre-clarification â†’ Less mid-debate input needed
- Mid-debate fills gaps that pre-clarification missed
- Both use agent-driven judgment (V3.0 philosophy)

### Round Loop (1-7 rounds, or until convergence)

**Each Round:**

A. **Role-Based Work**
   - Rounds 1-2 (Exploration): Claude diverges â†’ Codex reality-checks
   - Rounds 3+ (Convergence): Claude aligns â†’ Codex validates

B. **Facilitator Checks (Every Round)**
   - Anti-pattern detection â†’ AI escalates if needed
   - Scarcity check â†’ Abort if thresholds crossed
   - Coverage gaps â†’ Prompt agents
   - Mode re-evaluation â†’ Auto-switch if mismatch

C. **Stress Pass (Before Consensus)**
   - Last endorsing agent enumerates failure modes
   - Prevents rubber-stamping

D. **Escalation Conditions**
   - 7 rounds exceeded â†’ Flag to user
   - Facilitator prompts ignored 2+ times â†’ Hand control to user
   - High-friction patterns â†’ Immediate user escalation

### Post-Debate

1. **Quality Gate** (Mandatory)
   - Run checklist from `facilitator/quality-gate.md`
   - Block finalization if criteria not met

2. **Structured Logging**
   - Generate debate log (schema: `schemas/debate-log.json`)
   - Store for playbook pipeline
   - Include metrics: confidence, evidence tiers, coverage

3. **Report Generation**
   - Use template from V2.0 (still valid)
   - Add V3.0 enhancements:
     - Confidence levels
     - Coverage matrix
     - Evidence tier breakdown
     - Facilitator interventions log

4. **Cleanup**
   - Run debate-end.sh
   - Archive session

### Async (Background)

- **Nightly**: Playbook extraction from debate logs
- **Weekly**: User feedback integration â†’ heuristic tuning
- **Monthly**: Playbook validity re-validation

## Usage Examples

### Basic Usage with Clarification ğŸ†•

```
User: "codexì™€ í† ë¡ í•´ì„œ ì´ ì„±ëŠ¥ ë¬¸ì œ í•´ê²°í•´ì¤˜. DB ì¿¼ë¦¬ê°€ ëŠë ¤."

Claude (Clarification Stage):
- Analyzes: No tech stack, no target metrics, no constraints
- Generates 3 questions:

"ëª…í™•í™” ì§ˆë¬¸:
1. í˜„ì¬ ì‘ë‹µ ì‹œê°„ê³¼ ëª©í‘œ ì‘ë‹µ ì‹œê°„ì€?
2. ì‚¬ìš© ì¤‘ì¸ í”„ë ˆì„ì›Œí¬ì™€ ë°ì´í„°ë² ì´ìŠ¤ëŠ”?
3. ì˜ˆì‚°/ì‹œê°„/íŒ€ ì œì•½ì‚¬í•­ì€?"

User: "í˜„ì¬ í‰ê·  2ì´ˆ, ëª©í‘œ 500ms ì´í•˜. Django 4.2 + PostgreSQL 14.
       1ì£¼ì¼ ë‚´ ê°œì„ í•´ì•¼ í•˜ê³ , íŒ€ì— DBA ì—†ìŒ."

Claude (Now with full context):
1. Auto-detects mode: execution (urgent timeline)
2. Loads playbook: database-optimization.md
3. Initializes coverage with known constraints
4. Sets evidence tiers: High expectations (stack known)
5. Starts debate...

Round 1: Claude explores 3 approaches â†’ Codex reality-checks
Facilitator: Checks coverage - all critical dimensions addressed

Round 2: Solution converges (fast due to clear constraints)
Stress pass: Codex enumerates failure modes

Facilitator: Quality gate passes
- Clarification: âœ“ Complete
- Constraints: âœ“ Honored (no DBA, 1 week)
- Confidence: 85% (Tier 1 evidence from known stack)

Output: "1. Add B-tree index on users.email (immediate, no DBA needed)
         2. Use select_related() for N+1 queries (Django builtin)
         3. Connection pooling config (PgBouncer, 2 hours setup)
         Confidence: 85%. Can achieve 500ms target in 3 days."
```

### With Skip Clarify

```
User: "Django 4.2 + PostgreSQL 14 ì„±ëŠ¥ ê°œì„  (2ì´ˆâ†’500ms, 1ì£¼ì¼, DBA ì—†ìŒ)"

Claude:
- Analyzes: All constraints present
- Skips clarification automatically
- Proceeds directly to debate

(Same outcome as above, faster start)
```

### Explicit Mode Selection

```
User: "Use exploration mode - codexì™€ í•¨ê»˜ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ íŒ¨í„´ íƒìƒ‰í•´ì¤˜"

Claude:
1. Mode: exploration (5-7 rounds, creativity weighted)
2. No playbook (novel exploration)
3. Rounds 1-3: Divergent ideation
4. Rounds 4-5: Reality-check + synthesis
5. Output: Multiple options with tradeoffs, no single recommendation
```

### User Override

```
During debate:

User: "/debate-override Skip security discussion, focus on performance only"

Facilitator: Acknowledges override, updates coverage monitor
Debate continues with adjusted focus
```

## Scripts Location

V3.0 **reuses V2.0 scripts** for Codex interaction:
- `.claude/scripts/codex-debate/debate-start.sh`
- `.claude/scripts/codex-debate/debate-continue.sh`
- `.claude/scripts/codex-debate/debate-end.sh`

**New V3.0 scripts** (facilitator automation):
- `scripts/facilitator/check-coverage.py` (planned)
- `scripts/facilitator/detect-anti-patterns.py` (planned)
- `scripts/facilitator/generate-playbook.py` (planned)

## Configuration Files

### Facilitator Rules

- `facilitator/rules/coverage-monitor.yaml` - 8-dimension checklist
- `facilitator/rules/anti-patterns.yaml` - Detection patterns
- `facilitator/rules/scarcity-thresholds.yaml` - Abort conditions

### Mode Definitions

- `modes/exploration.yaml` - Creative mode settings
- `modes/balanced.yaml` - Default mode settings
- `modes/execution.yaml` - Fast execution settings

### Schemas

- `schemas/debate-log.json` - Structured log format
- `schemas/playbook-schema.json` - Playbook structure
- `schemas/metrics.json` - Quality metrics

## Comparison: V2.0 vs V3.0

See `references/v2-vs-v3-comparison.md` for detailed comparison.

**Quick Summary:**

| Aspect | V2.0 | V3.0 |
|--------|------|------|
| **Philosophy** | Facilitate debates | Ensure quality outcomes |
| **Quality Control** | Manual | Automated (3-layer facilitator) |
| **Adaptability** | One-size-fits-all | 3 modes (exploration/balanced/execution) |
| **Knowledge Reuse** | None | Automated playbook pipeline |
| **Failure Handling** | Hope for the best | 10 explicit mechanisms |
| **Evidence** | Implicit | Tiered with confidence levels |
| **Coverage** | Ad-hoc | 8-dimension systematic monitoring |
| **Consensus** | Accept at face value | Stress pass required |

## Best Practices

### Do's âœ…

1. **Answer Clarification Questions** ğŸ†• - Better info = better solutions
2. **Provide Complete Context Upfront** ğŸ†• - Skip clarify stage by being thorough initially
3. **Trust the Facilitator** - If it aborts for missing info, provide the info
4. **Specify Mode When Needed** - Exploration for novel problems, execution for urgent fixes
5. **Review Playbooks** - If loaded, verify it matches your context
6. **Check Confidence Levels** - Low confidence = validate before implementing
7. **Save Debate Reports** - Future playbooks depend on this data

### Don'ts âŒ

1. **Don't Rush Past Clarification** ğŸ†• - 2 minutes clarifying saves hours debugging
2. **Don't Say "I Don't Know" Without Details** ğŸ†• - "Don't know budget" vs "Budget TBD but likely <$10k"
3. **Don't Skip Quality Gate** - If it flags issues, address them
4. **Don't Ignore Facilitator Warnings** - They catch real problems
5. **Don't Expect Perfection** - V3.0 is designed to be explicit about uncertainty
6. **Don't Override Lightly** - Facilitator has reasons for its checks
7. **Don't Implement Low-Confidence Solutions** - Validate first

### Clarification Pro Tips ğŸ†•

**Good Initial Requests** (Auto-skip clarify):
- âœ… "Django 4.2 API ì„±ëŠ¥ ê°œì„  (2ì´ˆâ†’500ms, PostgreSQL 14, 1ì£¼ì¼, DBA ì—†ìŒ)"
- âœ… "Microservices vs Monolith (íŒ€ 5ëª…, ì‹ ê·œ í”„ë¡œì íŠ¸, Node.js, AWS, 3ê°œì›”)"

**Vague Requests** (Will trigger clarify):
- âŒ "API ë¹ ë¥´ê²Œ í•´ì¤˜"
- âŒ "ì–´ë–¤ ì•„í‚¤í…ì²˜ê°€ ì¢‹ì„ê¹Œ?"

**Clarification Responses**:
- âœ… "Django 4.2, PostgreSQL 14, AWS t3.medium, 1ì£¼ì¼ ë‚´, DBA ì—†ìŒ"
- âŒ "Django ì“°ê³  ìˆì–´ìš”" (Which version? DB? Constraints?)

## Troubleshooting

### "Facilitator keeps aborting"

Check scarcity detector output - you likely have insufficient information. Provide the requested data or acknowledge the decision must be made under uncertainty.

### "Debate stuck in exploration"

Mode might be misclassified. Use `/debate-override Switch to execution mode` to force convergence.

### "Playbook doesn't match my problem"

Playbooks are templates, not straitjackets. If mismatch is severe, use `/debate-override Ignore playbook` to proceed without it.

### "Quality gate blocking finalization"

Review checklist - one or more criteria not met. Either address the issue or document why it's acceptable to proceed without meeting it.

## Limitations

V3.0 Cannot:
- Replace human judgment (AI recommendation requires approval)
- Execute code or run tests (propose solutions, don't implement)
- Access external APIs or databases
- Guarantee "correct" answer (provides perspectives + confidence)
- Work offline (requires OpenAI API access)
- Function without Codex CLI installed

**Remember:** V3.0 is a tool for quality assurance and exploration, not a replacement for human decision-making.

## Meta-Learning

V3.0 improves itself through:
1. **Playbook accumulation** - Each debate potentially creates new playbooks
2. **Heuristic tuning** - User feedback adjusts facilitator thresholds
3. **Coverage expansion** - Telemetry discovers missing dimensions
4. **Mode calibration** - Success metrics refine mode selection

**Review cycle**: After 50 debates (~3-6 months), analyze metrics and update facilitator rules.

## References

- V2.0 Documentation: `~/.claude/skills/codex-collaborative-solver/skill.md`
- V3.0 Design Debate: `references/v3-design-debate.md`
- Comparison Guide: `references/v2-vs-v3-comparison.md`
- Facilitator Protocol: `facilitator/README.md`
- Playbook Guide: `playbooks/README.md`

## Appendix: Quick Start Checklist

**First Time Using V3.0:**
- [ ] Codex CLI installed and authenticated
- [ ] Read `references/v2-vs-v3-comparison.md`
- [ ] Understand 3 quality modes
- [ ] Know when facilitator will abort (scarcity thresholds)
- [ ] Aware of 8 coverage dimensions

**Before Each Debate:**
- [ ] Choose mode (or let auto-detect)
- [ ] Check if playbook exists for problem type
- [ ] Prepare to provide missing info if facilitator aborts

**After Each Debate:**
- [ ] Review confidence levels
- [ ] Validate low-confidence recommendations
- [ ] Check coverage matrix - any gaps?
- [ ] Provide feedback if quality unexpected

---

**Version:** 3.0.0
**Status:** Experimental (Test vs V2.0)
**Last Updated:** 2025-10-31
**Based On:** Meta-debate design session (Session ID: 019a37e1-6dc5-7d91-824e-52cae43772eb)
**Models:** Claude 3.5 Sonnet (Anthropic) + GPT-5 Codex via Codex CLI (OpenAI)
